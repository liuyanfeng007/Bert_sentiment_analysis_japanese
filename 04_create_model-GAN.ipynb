{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fc96deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "from transformers import DistilBertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b457c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 110\n",
    "output_file = \"data.tfrecord\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3832521",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\orignal_data.pkl\", \"rb\") as myprofile:  \n",
    "    dt = pickle.load(myprofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61224d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizer'.\n",
      "You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n",
      "Some layers from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing TFDistilBertForSequenceClassification: ['nsp___cls', 'mlm___cls', 'bert']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['distilbert', 'classifier', 'dropout_37', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking', num_labels=2)\n",
    "#.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4796e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output_file = \"train_data.tfrecord\"\n",
    "test_output_file = \"test_data.tfrecord\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e518f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_data_and_label_from_record(record):\n",
    "    x = {\n",
    "        \"input_ids\": record[\"input_ids\"],\n",
    "        \"input_mask\": record[\"input_mask\"],\n",
    "        # 'segment_ids': record['segment_ids']\n",
    "    }\n",
    "    y = record[\"label_ids\"]\n",
    "    return (x, y)\n",
    "def _decode_record(record, name_to_features):\n",
    "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "    return tf.io.parse_single_example(record, name_to_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d20b07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_data(file_name,isTrain = False):\n",
    "    dataset = tf.data.TFRecordDataset(file_name)\n",
    "    if isTrain :\n",
    "        dataset = dataset.repeat(500)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    name_to_features = {\n",
    "            \"input_ids\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "            \"input_mask\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "            # \"segment_ids\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "            \"label_ids\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        }\n",
    "    drop_remainder=False\n",
    "    dataset = dataset.apply(\n",
    "        tf.data.experimental.map_and_batch(\n",
    "            lambda record: _decode_record(record, name_to_features),\n",
    "            batch_size=100,\n",
    "            drop_remainder=drop_remainder,\n",
    "            num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "        )\n",
    "    )\n",
    "    dataset.cache()\n",
    "    re_dataset = dataset.map(select_data_and_label_from_record)\n",
    "    return re_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56b0ec68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\liuyf\\AppData\\Local\\Temp\\ipykernel_22936\\3084355475.py:18: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = create_train_test_data(train_output_file,True)\n",
    "test_dataset = create_train_test_data(test_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80000196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec=({'input_ids': TensorSpec(shape=(None, 110), dtype=tf.int64, name=None), 'input_mask': TensorSpec(shape=(None, 110), dtype=tf.int64, name=None)}, TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fce92022",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-3\n",
    "epsilon = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d6d84b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self,model):\n",
    "        super().__init__()\n",
    "        self.backup = {}\n",
    "        #self.model = model.distilbert(input_ids,input_mask)[0]\n",
    "        self.model = model\n",
    "        self.model.trainable = False\n",
    "        self.layersG = tf.keras.layers.GlobalMaxPool1D()\n",
    "        self.layers1 = tf.keras.layers.Dense(50, activation=\"relu\")\n",
    "        self.layersD2 = tf.keras.layers.Dropout(0.2)\n",
    "        self.layers3 = tf.keras.layers.Dense(10, activation=\"relu\")\n",
    "        self.layersD4 = tf.keras.layers.Dropout(0.2)\n",
    "        self.layers5 = tf.keras.layers.Dense(2, activation=\"softmax\")\n",
    "        self.Bidirectional=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(60, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))\n",
    "    def call(self, inputs):\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        input_mask = inputs[\"input_mask\"]\n",
    "        embedding_layer  = self.model.distilbert(input_ids,input_mask)[0]\n",
    "        X =self.Bidirectional(embedding_layer)\n",
    "        X = self.layersG(X)\n",
    "        X = self.layers1(X)\n",
    "        X = self.layersD2(X)\n",
    "        X = self.layers3(X)\n",
    "        X = self.layersD4(X)\n",
    "        X = self.layers5(X)\n",
    "        return X\n",
    "    def train_step(self, data):\n",
    "        if len(data) == 3:\n",
    "            x, y, sample_weight = data\n",
    "        else:\n",
    "            sample_weight = None\n",
    "            x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            # Compute the loss value.\n",
    "            # The loss function is configured in `compile()`.\n",
    "            loss = self.compiled_loss(\n",
    "                y,\n",
    "                y_pred,\n",
    "                sample_weight=sample_weight,\n",
    "                regularization_losses=self.losses,\n",
    "            )\n",
    "        #Tensorflow2.0 自定义模型训练实现NLP中的FGM对抗训练 代码实现\n",
    "        # Compute embedding gradients\n",
    "        embedding = self.trainable_variables[0]\n",
    "        embedding_gradients = tape.gradient(loss, embedding)[0]\n",
    "        embedding_gradients = tf.zeros_like(embedding) + embedding_gradients\n",
    "        delta = 0.01*embedding_gradients/(tf.math.sqrt(tf.reduce_sum(embedding_gradients**2))+1e-8)\n",
    "        self.trainable_variables[0].assign_add(delta)\n",
    "        \n",
    "        with tf.GradientTape() as tape2:\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            # Compute the loss value.\n",
    "            # The loss function is configured in `compile()`.\n",
    "            loss2 = self.compiled_loss(\n",
    "                y,\n",
    "                y_pred,\n",
    "                sample_weight=sample_weight,\n",
    "                regularization_losses=self.losses,\n",
    "            )\n",
    "            \n",
    "        gradients = tape2.gradient(loss2, self.trainable_variables)\n",
    "        self.trainable_variables[0].assign_sub(delta)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        # Update the metrics.\n",
    "        # Metrics are configured in `compile()`.\n",
    "        self.compiled_metrics.update_state(y, y_pred, sample_weight=sample_weight)\n",
    "\n",
    "        # Return a dict mapping metric names to current value.\n",
    "        # Note that it will include the loss (tracked in self.metrics).\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "        \n",
    "mode2 = MyModel(model)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy(\"accuracy\")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon)\n",
    "mode2.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b372440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "20/20 [==============================] - 1318s 65s/step - loss: 0.6908 - accuracy: 0.5795 - val_loss: 0.6788 - val_accuracy: 0.5900\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 1189s 60s/step - loss: 0.6791 - accuracy: 0.5905 - val_loss: 0.6672 - val_accuracy: 0.5900\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 1181s 59s/step - loss: 0.6579 - accuracy: 0.6160 - val_loss: 0.6480 - val_accuracy: 0.7660\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 1218s 61s/step - loss: 0.5993 - accuracy: 0.7095 - val_loss: 0.5652 - val_accuracy: 0.7700\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 1248s 63s/step - loss: 0.5022 - accuracy: 0.7570 - val_loss: 0.4874 - val_accuracy: 0.7920\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 1247s 63s/step - loss: 0.4463 - accuracy: 0.7775 - val_loss: 0.4340 - val_accuracy: 0.8040\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 1248s 63s/step - loss: 0.3877 - accuracy: 0.8025 - val_loss: 0.3951 - val_accuracy: 0.8200\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 1318s 66s/step - loss: 0.3523 - accuracy: 0.8670 - val_loss: 0.4653 - val_accuracy: 0.8160\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 1283s 64s/step - loss: 0.3211 - accuracy: 0.8690 - val_loss: 0.4221 - val_accuracy: 0.8000\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 1283s 65s/step - loss: 0.3107 - accuracy: 0.8845 - val_loss: 0.3628 - val_accuracy: 0.8380\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 1369s 69s/step - loss: 0.2444 - accuracy: 0.9140 - val_loss: 0.3796 - val_accuracy: 0.8500\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 1397s 70s/step - loss: 0.2175 - accuracy: 0.9305 - val_loss: 0.3652 - val_accuracy: 0.8600\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 1400s 70s/step - loss: 0.1940 - accuracy: 0.9365 - val_loss: 0.4171 - val_accuracy: 0.8440\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 1395s 70s/step - loss: 0.1839 - accuracy: 0.9495 - val_loss: 0.4590 - val_accuracy: 0.8360\n",
      "Epoch 15/50\n",
      " 9/20 [============>.................] - ETA: 11:13 - loss: 0.1803 - accuracy: 0.9567"
     ]
    }
   ],
   "source": [
    "history = mode2.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch = 20,\n",
    "    validation_data=test_dataset,\n",
    "    validation_steps=5,\n",
    "    #shuffle=True,\n",
    "    epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c84049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
